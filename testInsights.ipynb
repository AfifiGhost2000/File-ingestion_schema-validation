{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting testutility.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile testutility.py\n",
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import datetime \n",
    "import gc\n",
    "import re\n",
    "def read_config_file(filepath):\n",
    "    with open(filepath, 'r') as stream:\n",
    "        try:\n",
    "            return yaml.safe_load(stream)\n",
    "        except yaml.YAMLError as exc:\n",
    "            logging.error(exc)\n",
    "def replacer(string, char):\n",
    "    pattern = char + '{2,}'\n",
    "    string = re.sub(pattern, char, string) \n",
    "    return string\n",
    "\n",
    "def col_header_val(df,table_config):\n",
    "    '''\n",
    "    replace whitespaces in the column\n",
    "    and standardized column names\n",
    "    '''\n",
    "    df.columns = df.columns.astype(str).str.lower()\n",
    "    df.columns = df.columns.astype(str).str.replace('[^\\w]','_',regex=True)\n",
    "    df.columns = list(map(lambda x: x.strip('_'), list(df.columns)))\n",
    "    df.columns = list(map(lambda x: replacer(x,'_'), list(df.columns)))\n",
    "    expected_col = list(map(lambda x: x.lower(),  table_config['columns']))\n",
    "    expected_col.sort()\n",
    "    df.columns =list(map(lambda x: x.lower(), list(df.columns)))\n",
    "    df = df.reindex(sorted(df.columns), axis=1)\n",
    "    if len(df.columns) == len(expected_col) and list(expected_col)  == list(df.columns):\n",
    "        print(\"column name and column length validation passed\")\n",
    "        return 1\n",
    "    else:\n",
    "        print(\"column name and column length validation failed\")\n",
    "        mismatched_columns_file = list(set(df.columns).difference(expected_col))\n",
    "        print(\"Following File columns are not in the YAML file\",mismatched_columns_file)\n",
    "        missing_YAML_file = list(set(expected_col).difference(df.columns))\n",
    "        print(\"Following YAML columns are not in the file uploaded\",missing_YAML_file)\n",
    "        logging.info(f'df columns: {df.columns}')\n",
    "        logging.info(f'expected columns: {expected_col}')\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting file.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile file.yaml\n",
    "file_type: csv\n",
    "dataset_name: testfile\n",
    "file_name: chunk0\n",
    "table_name: edsurv\n",
    "inbound_delimiter: \",\"\n",
    "outbound_delimiter: \"|\"\n",
    "skip_leading_rows: 1\n",
    "columns: \n",
    "    - Client_id\n",
    "    - age_start_observed\n",
    "    - age_end\n",
    "    - is_truncated\n",
    "    - is_censored\n",
    "    - is_dead\n",
    "    - date_start_observed\n",
    "    - date_end_observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read config file\n",
    "import testutility as util\n",
    "from testutility import *\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import ray \n",
    "import modin.pandas as mpd\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import logging\n",
    "import gzip\n",
    "import shutil\n",
    "import codecs as codec\n",
    "from multiprocessing import Pool\n",
    "config_data = util.read_config_file(\"file.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file using config file\n",
    "class SourceFile():\n",
    "    file_type = config_data['file_type']\n",
    "    source_file = \"./csvchunks/\" + config_data['file_name'] + f'.{file_type}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1st Way to read csv file: Using PANDAS\n",
    "# start = time.time()\n",
    "\n",
    "# def sum_column(chunk, column_idx):\n",
    "#     return chunk.iloc[:,column_idx].sum()\n",
    "\n",
    "# chunksize = 10 ** 6\n",
    "# total_sum = 0\n",
    "# column_index = 0\n",
    "#df = pd.DataFrame\n",
    "# with pd.read_csv('survival_data.csv', chunksize=chunksize) as reader:\n",
    "#     for chunk in reader:\n",
    "#         total_sum += sum_column(chunk, column_index)\n",
    "#          df = pd.concat([chunk])\n",
    "\n",
    "# print(f\"Total rows: {total_sum}\")\n",
    "# print(df.head())\n",
    "# print(f\"Done in {time.time()-start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2nd Way to read csv file: Using DASK\n",
    "# %%time\n",
    "# dask_df = dd.read_csv('survival_data.csv', blocksize=25e6)\n",
    "# pandas_df = dask_df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3rd Way to read csv file: Using MODIN \n",
    "# %%time\n",
    "# os.environ[\"MODIN_ENGINE\"] = \"dask\"\n",
    "# from distributed import Client\n",
    "# client = Client(memory_limit='8GB')\n",
    "# df = mpd.read_csv('survival_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 18:06:29,486\tWARNING read_api.py:326 -- ⚠️  The number of blocks in this dataset (1) limits its parallelism to 1 concurrent tasks. This is much less than the number of available CPU slots in the cluster. Use `.repartition(n)` to increase the number of dataset blocks.\n",
      "Read progress: 100%|██████████| 1/1 [00:00<00:00, 309.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   client_id  age_start_observed    age_end  is_truncated  is_censored  \\\n",
      "0   15113102            0.000000   9.097335         False         True   \n",
      "1   41505894            0.000000  64.486689         False         True   \n",
      "2   24774171            0.000000  33.071552         False         True   \n",
      "3   97834936           34.834566  68.778258          True         True   \n",
      "4   45793809            0.000000  95.948358         False        False   \n",
      "\n",
      "   is_dead date_start_observed date_end_observed  \n",
      "0    False          1908-11-17        1917-12-22  \n",
      "1    False          1828-09-13        1893-03-10  \n",
      "2    False          1911-02-07        1944-03-04  \n",
      "3    False          1820-01-01        1853-12-10  \n",
      "4     True          1870-05-29        1966-05-11  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "FutureWarning: In a future version of pandas all arguments of DataFrame.dropna will be keyword-only.\n"
     ]
    }
   ],
   "source": [
    "#4th Way to read csv file: Using RAY\n",
    "#%%time\n",
    "#for reading the whole csv file, it doesn't return ray dataset but list object\n",
    "#df = ray.data.read_csv(SourceFile.source_file).window(blocks_per_window=12).split(100, equal=True)\n",
    "df = ray.data.read_csv(SourceFile.source_file)\n",
    "df = df.to_pandas()\n",
    "df = df.rename(columns={'Unnamed: 0' : 'client_id'})\n",
    "\n",
    "df.dropna(0, inplace=True)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "#df = pd.read_csv('test.csv')\n",
    "\n",
    "#for taking into consideration all cvs file chunks use glob\n",
    "# import glob\n",
    "# PATH = 'C:/Users/abdul/Documents/FileIngestionAndSchemaValidation/csvchunks'\n",
    "# all_files = glob.glob(os.path.join(PATH, \"*.csv\"))     \n",
    "\n",
    "# df_from_each_file = (pd.read_csv(f) for f in all_files)\n",
    "# concatenated_df   = pd.concat(df_from_each_file, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV File Already segregated into chunks!\n"
     ]
    }
   ],
   "source": [
    "PATH = 'C:/Users/abdul/Documents/FileIngestionAndSchemaValidation/csvchunks'\n",
    "isdir = os.path.isdir(PATH)\n",
    "if(isdir):\n",
    "     print('CSV File Already segregated into chunks!')\n",
    "else:\n",
    "     for i,chunk in enumerate(pd.read_csv(SourceFile.source_file, chunksize=40000)):\n",
    "          chunk.to_csv('csvchunks/chunk{}.csv'.format(i), index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##total of 2220 chunks generated!! ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column name and column length validation passed\n",
      "Index(['client_id', 'age_start_observed', 'age_end', 'is_truncated',\n",
      "       'is_censored', 'is_dead', 'date_start_observed', 'date_end_observed'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "util.col_header_val(df,config_data)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns of files are: Index(['client_id', 'age_start_observed', 'age_end', 'is_truncated',\n",
      "       'is_censored', 'is_dead', 'date_start_observed', 'date_end_observed'],\n",
      "      dtype='object')\n",
      "columns of YAML are: ['Client_id', 'age_start_observed', 'age_end', 'is_truncated', 'is_censored', 'is_dead', 'date_start_observed', 'date_end_observed']\n"
     ]
    }
   ],
   "source": [
    "print(\"columns of files are:\" ,df.columns)\n",
    "print(\"columns of YAML are:\" ,config_data['columns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "def convert_to_gzip(source_file):\n",
    "\n",
    "    print(\"File {} is being Converted\".format(config_data['file_name']))\n",
    "\n",
    "    df.to_csv('temp.txt', sep = config_data['outbound_delimiter'], index=False)\n",
    "\n",
    "    f_in = open('temp.txt')\n",
    "    f_out = gzip.open('test.gz', 'wb')\n",
    "\n",
    "    #csv_w = csv.writer(f_out)\n",
    "\n",
    "    for row in f_in:\n",
    "        f_out.write(base64.encodebytes(row.encode()))\n",
    "\n",
    "    \n",
    "    f_out.close()\n",
    "    f_in.close()\n",
    "\n",
    "    ##Used multiprocessing on the 2220 chunck csv file to convert large csv file to gz format efficiently\n",
    "    # with Pool() as pool:\n",
    "    #     result = pool.map(convert_to_gzip, os.listdir(PATH))\n",
    "\n",
    "    # return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column name and column length validation passed\n",
      "col validation passed\n",
      "File chunk0 is being Converted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if util.col_header_val(df,config_data)==0:\n",
    "    print(\"validation failed\")\n",
    "    # write code to reject the file\n",
    "else:\n",
    "    print(\"col validation passed\")\n",
    "    convert_to_gzip(SourceFile.source_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse csv file just for testing purposes\n",
    "# def parseCSVfile():\n",
    "#     with open('test.csv', newline='') as infile:\n",
    "#         reader = csv.reader(infile, dialect = 'excel')\n",
    "#         with open('temp.txt', mode='w') as outfile:        \n",
    "#             writer = csv.writer(outfile, delimiter= config_data['outbound_delimiter'])\n",
    "#             writer.writerows(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parseCSVfile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Another way for converting to gz format but this takes a lot of time\n",
    "# file_type = 'gz'\n",
    "# with gzip.open(source_file, \"wt\") as f:\n",
    "#     file_type = 'csv'\n",
    "#     reader =  open(source_file, 'rt') \n",
    "\n",
    "#     writer =csv.writer(f)\n",
    "#     for row in reader:\n",
    "#         writer.writerow(codec.encode(row))\n",
    "\n",
    "# f.close()\n",
    "# reader.close()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time Statistics of dataframe reader\n",
    "#1. pandas = 114.24626564979553 seconds\n",
    "#2. dask = (Wall time) 2 min 56 s\n",
    "#3. ray = (Wall time) 1 min 30 s\n",
    "#4. modin = took much longer than expected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File Summary\n",
    "#Total no. rows: 3946661545 (~approx 4 Billion rows)\n",
    "#Total no. cols: 7\n",
    "#File size: 6.07 GB (6,525,517,581 bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ea3a11aaa978d9646058336c84c9a0218aba8c5d679a7a8e354537b3f287648"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
